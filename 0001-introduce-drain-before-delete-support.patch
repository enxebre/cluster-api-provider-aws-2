From 6323038d4a1ff5170f049aab6beca2c634d594b9 Mon Sep 17 00:00:00 2001
From: Enxebre <alberto.garcial@hotmail.com>
Date: Tue, 2 Oct 2018 08:50:45 +0200
Subject: [PATCH] introduce drain before delete support

---
 .../pkg/controller/machine/controller.go           |  29 ++
 .../cluster-api/pkg/controller/machine/drain.go    | 465 +++++++++++++++++++++
 2 files changed, 494 insertions(+)
 create mode 100644 vendor/sigs.k8s.io/cluster-api/pkg/controller/machine/drain.go

diff --git a/vendor/sigs.k8s.io/cluster-api/pkg/controller/machine/controller.go b/vendor/sigs.k8s.io/cluster-api/pkg/controller/machine/controller.go
index e909d64d..480d5c93 100644
--- a/vendor/sigs.k8s.io/cluster-api/pkg/controller/machine/controller.go
+++ b/vendor/sigs.k8s.io/cluster-api/pkg/controller/machine/controller.go
@@ -18,6 +18,7 @@ package machine
 
 import (
 	"errors"
+	"bytes"
 	"fmt"
 	"os"
 	"time"
@@ -107,7 +108,35 @@ func (c *MachineControllerImpl) Reconcile(machine *clusterv1.Machine) error {
 			glog.Infof("Skipping reconciling of machine object %v", name)
 			return nil
 		}
+
 		glog.Infof("reconciling machine object %v triggers delete.", name)
+		if machine.Labels["force-deletion"] != "True" && machine.Status.NodeRef != nil {
+			glog.Infof("draining node before delete")
+			buf := bytes.NewBuffer([]byte{})
+			errBuf := bytes.NewBuffer([]byte{})
+
+			nodeName := machine.Status.NodeRef.Name
+			drainOptions := NewDrainOptions(
+				c.kubernetesClientSet,
+				nodeName,
+				-1,
+				true,
+				true,
+				true,
+				buf,
+				errBuf,
+			)
+			err := drainOptions.RunDrain()
+			if err != nil {
+				// Machine still tries to terminate after drain failure
+				glog.Warningf("drain failed for machine %s - \nBuf:%v \nErrBuf:%v \nErr-Message:%v", machine.Name, buf, errBuf, err)
+				return err
+			}
+			glog.Infof("drain successful for machine %s - %v %v", machine.Name, buf, errBuf)
+		} else {
+			glog.Infof("found force-deletion label, skipping node drain")
+		}
+
 		if err := c.delete(m); err != nil {
 			glog.Errorf("Error deleting machine object %v; %v", name, err)
 			return err
diff --git a/vendor/sigs.k8s.io/cluster-api/pkg/controller/machine/drain.go b/vendor/sigs.k8s.io/cluster-api/pkg/controller/machine/drain.go
new file mode 100644
index 00000000..d3a7bd81
--- /dev/null
+++ b/vendor/sigs.k8s.io/cluster-api/pkg/controller/machine/drain.go
@@ -0,0 +1,465 @@
+/*
+Copyright 2018 The Kubernetes Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package machine
+
+import (
+	"errors"
+	"fmt"
+	"time"
+	"io"
+	"math"
+	"strings"
+
+	"github.com/golang/glog"
+	api "k8s.io/api/core/v1"
+	corev1 "k8s.io/api/core/v1"
+	policy "k8s.io/api/policy/v1beta1"
+	apierrors "k8s.io/apimachinery/pkg/api/errors"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/apimachinery/pkg/fields"
+	"k8s.io/apimachinery/pkg/util/wait"
+	"k8s.io/client-go/kubernetes"
+)
+
+const (
+	// EvictionKind is the kind used for eviction
+	EvictionKind = "Eviction"
+	// EvictionSubresource is the kind used for evicting pods
+	EvictionSubresource = "pods/eviction"
+
+	// Interval is the default Poll interval
+	Interval = time.Second * 1
+
+	daemonsetFatal      = "DaemonSet-managed pods (use --ignore-daemonsets to ignore)"
+	daemonsetWarning    = "Ignoring DaemonSet-managed pods"
+	localStorageFatal   = "pods with local storage (use --delete-local-data to override)"
+	localStorageWarning = "Deleting pods with local storage"
+	unmanagedFatal      = "pods not managed by ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet (use --force to override)"
+	unmanagedWarning    = "Deleting pods not managed by ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet"
+)
+
+// DrainOptions are configurable options while draining a node before deletion
+type DrainOptions struct {
+	client             kubernetes.Interface
+	Force              bool
+	GracePeriodSeconds int
+	IgnoreDaemonsets   bool
+	Timeout            time.Duration
+	DeleteLocalData    bool
+	nodeName           string
+	Out                io.Writer
+	ErrOut             io.Writer
+}
+
+// NewDrainOptions creates a new DrainOptions struct and returns a pointer to it
+func NewDrainOptions(
+	client kubernetes.Interface,
+	nodeName string,
+	gracePeriodSeconds int,
+	force bool,
+	ignoreDaemonsets bool,
+	deleteLocalData bool,
+	out io.Writer,
+	errOut io.Writer,
+) *DrainOptions {
+
+	return &DrainOptions{
+		client:             client,
+		Force:              force,
+		GracePeriodSeconds: gracePeriodSeconds,
+		IgnoreDaemonsets:   ignoreDaemonsets,
+		DeleteLocalData:    deleteLocalData,
+		nodeName:           nodeName,
+		Out:                out,
+		ErrOut:             errOut,
+	}
+
+}
+
+// RunDrain runs the 'drain' command
+func (o *DrainOptions) RunDrain() error {
+	if err := o.RunCordonOrUncordon(true); err != nil {
+		return err
+	}
+
+	err := o.deleteOrEvictPodsSimple()
+	return err
+}
+
+func (o *DrainOptions) deleteOrEvictPodsSimple() error {
+	pods, err := o.getPodsForDeletion()
+	if err != nil {
+		return err
+	}
+
+	err = o.deleteOrEvictPods(pods)
+	if err != nil {
+		pendingPods, newErr := o.getPodsForDeletion()
+		if newErr != nil {
+			return newErr
+		}
+		fmt.Fprintf(o.ErrOut, "There are pending pods when an error occurred: %v\n", err)
+		for _, pendingPod := range pendingPods {
+			fmt.Fprintf(o.ErrOut, "%s/%s\n", "pod", pendingPod.Name)
+		}
+	}
+	return err
+}
+
+// deleteOrEvictPods deletes or evicts the pods on the api server
+func (o *DrainOptions) deleteOrEvictPods(pods []api.Pod) error {
+	if len(pods) == 0 {
+		return nil
+	}
+
+	policyGroupVersion, err := SupportEviction(o.client)
+	if err != nil {
+		return err
+	}
+
+	getPodFn := func(namespace, name string) (*api.Pod, error) {
+		return o.client.CoreV1().Pods(namespace).Get(name, metav1.GetOptions{})
+	}
+
+	if len(policyGroupVersion) > 0 {
+		return o.evictPods(pods, policyGroupVersion, getPodFn)
+	}
+	return o.deletePods(pods, getPodFn)
+}
+
+func (o *DrainOptions) evictPods(pods []api.Pod, policyGroupVersion string, getPodFn func(namespace, name string) (*api.Pod, error)) error {
+	doneCh := make(chan bool, len(pods))
+	errCh := make(chan error, 1)
+
+	glog.Info("The following pods are being evicted -")
+
+	for _, pod := range pods {
+		go func(pod api.Pod, doneCh chan bool, errCh chan error) {
+			var err error
+			glog.Info("\t", pod.Name)
+			for {
+				err = o.evictPod(pod, policyGroupVersion)
+				if err == nil {
+					break
+				} else if apierrors.IsNotFound(err) {
+					glog.Infof("\t", pod.Name, " evicted")
+					doneCh <- true
+					return
+				} else if apierrors.IsTooManyRequests(err) {
+					time.Sleep(5 * time.Second)
+				} else {
+					errCh <- fmt.Errorf("error when evicting pod %q: %v", pod.Name, err)
+					return
+				}
+			}
+			podArray := []api.Pod{pod}
+			glog.Infof("Waiting for pod %s to be deleted", pod.Name)
+			_, err = o.waitForDelete(podArray, Interval, time.Duration(math.MaxInt64), true, getPodFn)
+			if err == nil {
+				glog.Infof("Deleted pod %s", pod.Name)
+				doneCh <- true
+			} else {
+				errCh <- fmt.Errorf("error when waiting for pod %q terminating: %v", pod.Name, err)
+			}
+		}(pod, doneCh, errCh)
+	}
+
+	doneCount := 0
+	// 0 timeout means infinite, we use MaxInt64 to represent it.
+	var globalTimeout time.Duration
+	if o.Timeout == 0 {
+		globalTimeout = time.Duration(math.MaxInt64)
+	} else {
+		globalTimeout = o.Timeout
+	}
+	for {
+		select {
+		case err := <-errCh:
+			return err
+		case <-doneCh:
+			doneCount++
+			if doneCount == len(pods) {
+				return nil
+			}
+		case <-time.After(globalTimeout):
+			return fmt.Errorf("Drain did not complete within %v", globalTimeout)
+		}
+	}
+}
+
+func (o *DrainOptions) evictPod(pod api.Pod, policyGroupVersion string) error {
+	deleteOptions := &metav1.DeleteOptions{}
+	if o.GracePeriodSeconds >= 0 {
+		gracePeriodSeconds := int64(o.GracePeriodSeconds)
+		deleteOptions.GracePeriodSeconds = &gracePeriodSeconds
+	}
+	eviction := &policy.Eviction{
+		TypeMeta: metav1.TypeMeta{
+			APIVersion: policyGroupVersion,
+			Kind:       EvictionKind,
+		},
+		ObjectMeta: metav1.ObjectMeta{
+			Name:      pod.Name,
+			Namespace: pod.Namespace,
+		},
+		DeleteOptions: deleteOptions,
+	}
+	// Remember to change change the URL manipulation func when Evction's version change
+	return o.client.PolicyV1beta1().Evictions(eviction.Namespace).Evict(eviction)
+}
+
+func (o *DrainOptions) deletePods(pods []api.Pod, getPodFn func(namespace, name string) (*api.Pod, error)) error {
+	// 0 timeout means infinite, we use MaxInt64 to represent it.
+	var globalTimeout time.Duration
+	if o.Timeout == 0 {
+		globalTimeout = time.Duration(math.MaxInt64)
+	} else {
+		globalTimeout = o.Timeout
+	}
+	for _, pod := range pods {
+		err := o.deletePod(pod)
+		if err != nil && !apierrors.IsNotFound(err) {
+			return err
+		}
+	}
+	_, err := o.waitForDelete(pods, Interval, globalTimeout, false, getPodFn)
+	return err
+}
+
+func (o *DrainOptions) deletePod(pod api.Pod) error {
+	deleteOptions := &metav1.DeleteOptions{}
+	if o.GracePeriodSeconds >= 0 {
+		gracePeriodSeconds := int64(o.GracePeriodSeconds)
+		deleteOptions.GracePeriodSeconds = &gracePeriodSeconds
+	}
+	return o.client.CoreV1().Pods(pod.Namespace).Delete(pod.Name, deleteOptions)
+}
+
+func (o *DrainOptions) waitForDelete(pods []api.Pod, interval, timeout time.Duration, usingEviction bool, getPodFn func(string, string) (*api.Pod, error)) ([]api.Pod, error) {
+	err := wait.PollImmediate(interval, timeout, func() (bool, error) {
+		pendingPods := []api.Pod{}
+		for i, pod := range pods {
+			p, err := getPodFn(pod.Namespace, pod.Name)
+			if apierrors.IsNotFound(err) || (p != nil && p.ObjectMeta.UID != pod.ObjectMeta.UID) {
+				//cmdutil.PrintSuccess(o.mapper, false, o.Out, "pod", pod.Name, false, verbStr)
+				//glog.Info("pod deleted successfully found")
+				continue
+			} else if err != nil {
+				return false, err
+			} else {
+				pendingPods = append(pendingPods, pods[i])
+			}
+		}
+		pods = pendingPods
+		if len(pendingPods) > 0 {
+			return false, nil
+		}
+		return true, nil
+	})
+	return pods, err
+}
+
+// SupportEviction uses Discovery API to find out if the server support eviction subresource
+// If support, it will return its groupVersion; Otherwise, it will return ""
+func SupportEviction(clientset kubernetes.Interface) (string, error) {
+	discoveryClient := clientset.Discovery()
+	groupList, err := discoveryClient.ServerGroups()
+	if err != nil {
+		return "", err
+	}
+	foundPolicyGroup := false
+	var policyGroupVersion string
+	for _, group := range groupList.Groups {
+		if group.Name == "policy" {
+			foundPolicyGroup = true
+			policyGroupVersion = group.PreferredVersion.GroupVersion
+			break
+		}
+	}
+	if !foundPolicyGroup {
+		return "", nil
+	}
+	resourceList, err := discoveryClient.ServerResourcesForGroupVersion("v1")
+	if err != nil {
+		return "", err
+	}
+	for _, resource := range resourceList.APIResources {
+		if resource.Name == EvictionSubresource && resource.Kind == EvictionKind {
+			return policyGroupVersion, nil
+		}
+	}
+	return "", nil
+}
+
+// RunCordonOrUncordon runs either Cordon or Uncordon.  The desired value for
+// "Unschedulable" is passed as the first arg.
+func (o *DrainOptions) RunCordonOrUncordon(desired bool) error {
+	node, err := o.client.CoreV1().Nodes().Get(o.nodeName, metav1.GetOptions{})
+	if err != nil {
+		// Deletion could be triggered when machine is just being created, no node present then
+		return nil
+	}
+	unsched := node.Spec.Unschedulable
+	if unsched == desired {
+		glog.Info("Already desired")
+	} else {
+		clone := node.DeepCopy()
+		clone.Spec.Unschedulable = desired
+
+		_, err = o.client.CoreV1().Nodes().Update(clone)
+		if err != nil {
+			return err
+		}
+	}
+	return nil
+}
+
+// Map of status message to a list of pod names having that status.
+type podStatuses map[string][]string
+
+func (ps podStatuses) Message() string {
+	msgs := []string{}
+
+	for key, pods := range ps {
+		msgs = append(msgs, fmt.Sprintf("%s: %s", key, strings.Join(pods, ", ")))
+	}
+	return strings.Join(msgs, "; ")
+}
+
+// Takes a pod and returns a bool indicating whether or not to operate on the
+// pod, an optional warning message, and an optional fatal error.
+type podFilter func(api.Pod) (include bool, w *warning, f *fatal)
+type warning struct {
+	string
+}
+type fatal struct {
+	string
+}
+
+func mirrorPodFilter(pod api.Pod) (bool, *warning, *fatal) {
+	if _, found := pod.ObjectMeta.Annotations[corev1.MirrorPodAnnotationKey]; found {
+		return false, nil, nil
+	}
+	return true, nil, nil
+}
+
+
+func hasLocalStorage(pod api.Pod) bool {
+	for _, volume := range pod.Spec.Volumes {
+		if volume.EmptyDir != nil {
+			return true
+		}
+	}
+
+	return false
+}
+
+func (o *DrainOptions) localStorageFilter(pod corev1.Pod) (bool, *warning, *fatal) {
+	if !hasLocalStorage(pod) {
+		return true, nil, nil
+	}
+	// Any finished pod can be removed.
+	if pod.Status.Phase == corev1.PodSucceeded || pod.Status.Phase == corev1.PodFailed {
+		return true, nil, nil
+	}
+	if !o.DeleteLocalData {
+		return false, nil, &fatal{localStorageFatal}
+	}
+
+	return true, &warning{localStorageWarning}, nil
+}
+
+
+// getPodsForDeletion returns all the pods we're going to delete.  If there are
+// any pods preventing us from deleting, we return that list in an error.
+func (o *DrainOptions) getPodsForDeletion() (pods []api.Pod, err error) {
+	podList, err := o.client.CoreV1().Pods(metav1.NamespaceAll).List(metav1.ListOptions{
+		FieldSelector: fields.SelectorFromSet(fields.Set{"spec.nodeName": o.nodeName}).String()})
+	if err != nil {
+		return pods, err
+	}
+
+	ws := podStatuses{}
+	fs := podStatuses{}
+
+	for _, pod := range podList.Items {
+		podOk := true
+		for _, filt := range []podFilter{mirrorPodFilter, o.localStorageFilter, o.unreplicatedFilter, o.daemonsetFilter} {
+			filterOk, w, f := filt(pod)
+
+			podOk = podOk && filterOk
+			if w != nil {
+				ws[w.string] = append(ws[w.string], pod.Name)
+			}
+			if f != nil {
+				fs[f.string] = append(fs[f.string], pod.Name)
+			}
+		}
+		if podOk {
+			pods = append(pods, pod)
+		}
+	}
+
+	if len(fs) > 0 {
+		return []api.Pod{}, errors.New(fs.Message())
+	}
+	if len(ws) > 0 {
+		fmt.Fprintf(o.ErrOut, "WARNING: %s\n", ws.Message())
+	}
+	return pods, nil
+}
+
+func (o *DrainOptions) unreplicatedFilter(pod api.Pod) (bool, *warning, *fatal) {
+	// any finished pod can be removed
+	if pod.Status.Phase == api.PodSucceeded || pod.Status.Phase == api.PodFailed {
+		return true, nil, nil
+	}
+
+	controllerRef := o.getPodController(pod)
+	if controllerRef != nil {
+		return true, nil, nil
+	}
+	if !o.Force {
+		return false, nil, &fatal{unmanagedFatal}
+	}
+	return true, &warning{unmanagedWarning}, nil
+}
+
+func (o *DrainOptions) getPodController(pod api.Pod) *metav1.OwnerReference {
+	return metav1.GetControllerOf(&pod)
+}
+
+func (o *DrainOptions) daemonsetFilter(pod api.Pod) (bool, *warning, *fatal) {
+	// Note that we return false in cases where the pod is DaemonSet managed,
+	// regardless of flags.  We never delete them, the only question is whether
+	// their presence constitutes an error.
+	//
+	// The exception is for pods that are orphaned (the referencing
+	// management resource - including DaemonSet - is not found).
+	// Such pods will be deleted if --force is used.
+	controllerRef := o.getPodController(pod)
+	if controllerRef == nil || controllerRef.Kind != "DaemonSet" {
+		return true, nil, nil
+	}
+	if _, err := o.client.ExtensionsV1beta1().DaemonSets(pod.Namespace).Get(controllerRef.Name, metav1.GetOptions{}); err != nil {
+		return false, nil, &fatal{err.Error()}
+	}
+	if !o.IgnoreDaemonsets {
+		return false, nil, &fatal{daemonsetFatal}
+	}
+	return false, &warning{daemonsetWarning}, nil
+}
-- 
2.15.2 (Apple Git-101.1)

